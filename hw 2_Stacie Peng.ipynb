{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48977792-b691-4fb3-9457-babca0e1d9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, col, upper, substring, current_date, datediff, floor, year, lit\n",
    "from pyspark.sql.functions import min as min_, max as max_, sum as sum_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5b807a-e57b-49c8-82e4-5ab10f8b1dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pitstops = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df79cb6-b407-4a44-a1d7-6e84740cc006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pitstops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3177b5b3-b8b6-4f27-b7f9-0f0c51b735b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate average pit stop time per driver per race\n",
    "avg_pit_time = df_pitstops.groupBy(\"raceId\", \"driverId\") \\\n",
    "                           .agg(avg(\"milliseconds\").alias(\"avg_pit_stop_time\")) \\\n",
    "                           .orderBy(\"raceId\", \"driverId\")\n",
    "\n",
    "display(avg_pit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b1d0e0-3054-4c0a-8f8f-81a087165251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header = True)\n",
    "\n",
    "df_pitstops = df_pitstops.withColumn(\"milliseconds\", df_pitstops[\"milliseconds\"].cast(\"int\"))\n",
    "df_results = df_results.withColumn(\"positionOrder\", df_results[\"positionOrder\"].cast(\"int\"))\n",
    "\n",
    "# Calculate average pit stop time per driver per race\n",
    "avg_pit_stop_time_df = df_pitstops.groupBy(\"raceId\", \"driverId\") \\\n",
    "                                   .agg(avg(\"milliseconds\").alias(\"avg_pit_stop_time_ms\"))\n",
    "\n",
    "# Join average pit stop time with race results to get finishing order\n",
    "joined_df = avg_pit_stop_time_df.join(df_results.select(\"raceId\", \"driverId\", \"positionOrder\"), \n",
    "                                     [\"raceId\", \"driverId\"])\n",
    "\n",
    "#  Rank drivers based on finishing order\n",
    "window_spec = Window.partitionBy(\"raceId\").orderBy(\"positionOrder\")\n",
    "\n",
    "ranked_df = joined_df.withColumn(\"finishing_rank\", rank().over(window_spec))\n",
    "\n",
    "#  Order results by race and rank\n",
    "final_df = ranked_df.orderBy(\"raceId\", \"finishing_rank\")\n",
    "\n",
    "display(final_df.select(\"raceId\", \"driverId\", \"positionOrder\", \"avg_pit_stop_time_ms\", \"finishing_rank\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dca30fdf-37fc-4f56-add2-a0f2de8085cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I chose to include all drivers initially but kept DNF drivers' positionOrder values in the dataset. For ranking, they naturally receive a lower finishing rank (since valid positionOrder values are ordered first). However, for certain analyses, excluding DNFs may make the insights cleaner, especially if you're interested in pit stop efficiency influencing race completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d4fa1d-1d99-4249-80d4-93e05de3741f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load drivers dataset\n",
    "df_drivers = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header = True)\n",
    "\n",
    "# Check current codes and where codes are missing\n",
    "df_drivers.select(\"driverId\", \"surname\", \"code\").filter(col(\"code\").isNull()).show()\n",
    "\n",
    "# Insert missing codes:\n",
    "# Approach: Use the first three letters of the driver's surname (uppercase) as the default code\n",
    "drivers_df_filled = df_drivers.withColumn(\n",
    "    \"code\",\n",
    "    when(col(\"code\").isNull(), upper(substring(col(\"surname\"), 1, 3)))  # First 3 letters of surname, uppercase\n",
    "    .otherwise(col(\"code\"))\n",
    ")\n",
    "\n",
    "# View updated dataframe\n",
    "display(df_drivers.select(\"driverId\", \"surname\", \"code\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a84895-4fd1-41d9-8915-76cc845c6a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert dob column to DateType\n",
    "df_drivers = df_drivers.withColumn(\"dob\", col(\"dob\").cast(\"date\"))\n",
    "\n",
    "# Create 'Age' column: Age = difference between current date and dob divided by 365\n",
    "# floor ensures we count full birthdays only (i.e., complete years)\n",
    "df_drivers = df_drivers.withColumn(\"Age\", floor(datediff(current_date(), col(\"dob\")) / 365))\n",
    "\n",
    "# Show sample of Age column\n",
    "df_drivers.select(\"driverId\", \"dob\", \"Age\").show()\n",
    "\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header = True)\n",
    "# Join results with drivers to include Age\n",
    "race_driver_df = df_results.join(df_drivers.select(\"driverId\", \"Age\"), on=\"driverId\")\n",
    "\n",
    "# Group by raceId and find min and max Age\n",
    "youngest_df = race_driver_df.groupBy(\"raceId\").agg(min_(\"Age\").alias(\"youngest_age\"))\n",
    "oldest_df = race_driver_df.groupBy(\"raceId\").agg(max_(\"Age\").alias(\"oldest_age\"))\n",
    "\n",
    "# Show youngest and oldest drivers per race\n",
    "display(youngest_df)\n",
    "display(oldest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091a4545-27c1-4679-b20a-278dc3a6ba79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header = True)\n",
    "df_results = df_results.withColumn(\"raceId\", col(\"raceId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"driverId\", col(\"driverId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"positionOrder\", col(\"positionOrder\").cast(\"int\"))\n",
    "\n",
    "df_results = df_results.withColumn(\"win_flag\", when(col(\"positionOrder\") == 1, 1).otherwise(0)) \\\n",
    "                       .withColumn(\"loss_flag\", when((col(\"positionOrder\") > 1) & (col(\"statusId\").contains(\"Finished\")), 1).otherwise(0))\n",
    "# Window specification: partition by driver, ordered by race\n",
    "window_spec = Window.partitionBy(\"driverId\").orderBy(\"raceId\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# Cumulative wins and losses before each race\n",
    "df_results = df_results.withColumn(\"cumulative_wins\", sum_(\"win_flag\").over(window_spec)) \\\n",
    "                       .withColumn(\"cumulative_losses\", sum_(\"loss_flag\").over(window_spec))\n",
    "\n",
    "display(df_results.select(\"raceId\", \"driverId\", \"cumulative_wins\", \"cumulative_losses\").orderBy(\"raceId\", \"driverId\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402c1a92-18de-4676-bc59-4f3a0eb14a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question: Which driver has the fastest lap on average?\n",
    "df_laptimes = spark.read.csv('s3://columbia-gr5069-main/raw/lap_times.csv', header = True)\n",
    "\n",
    "\n",
    "fastest_lap_df = df_laptimes.groupBy(\"driverId\") \\\n",
    "                             .agg(avg(\"milliseconds\").alias(\"avg_lap_time\")) \\\n",
    "                             .orderBy(\"avg_lap_time\")\n",
    "\n",
    "display(fastest_lap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7ab9b8-f025-4734-b8a7-6fdc41450d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw 2_Stacie Peng",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
